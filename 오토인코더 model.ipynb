{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator\n",
    "\n",
    "keras에 넣을려고 만든 generator입니다\n",
    "1. data: numpy array꼴\n",
    "2. lookback: timestep\n",
    "3. min_index, max_index: train data 안에서 validation data를 따로 만들어서 써보고 싶을때 사용. 통째로 train하려면 max_index = None\n",
    "4. batch_size: batch_size\n",
    "5. step=1, delay=0: autoencoder에서는 그냥 기본값으로.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, min_index, max_index, batch_size,\n",
    "             shuffle=False, step=1, delay=0):\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay -1\n",
    "        \n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index+lookback, max_index, size=batch_size)\n",
    "    # min_index+lookback과 max_index 사이의 숫자를 batch_size만큼 만든다.\n",
    "    # 단순히 batch를 만들어주기 위해 index를 rows에 저장  \n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            \n",
    "            rows = np.arrange(i, min(i+batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        \n",
    "        samples = np.zeros((len(rows), lookback//step , data.shape[-1]))\n",
    "        targets = np.zeros((len(rows), lookback//step , data.shape[-1]))\n",
    "        # shape = (batchsize, lookback, feature 수)\n",
    "        \n",
    "        for j, row in enumerate(rows):# j 는 0~batchsize, row는 index number\n",
    "            indices = range(rows[j]-lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[indices]\n",
    "        \n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑에 train_np는 normalization 다 시키고, numpy 배열화 한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(train_np, lookback = 90, min_index = 0, max_index=700000, batch_size= 200, shuffle=True)\n",
    "valid_gen = generator(train_np, lookback = 90, min_index = 700001, max_index=None, batch_size= 200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps = ((len(train_np) -1) - 700001 - 90) // 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder + lstm\n",
    "\n",
    "모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 90\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.LSTM(128, activation='tanh',\n",
    "                input_shape=(ts,79), return_sequences=True))\n",
    "model.add(layers.LSTM(64, activation='tanh', return_sequences=False))\n",
    "## encoder\n",
    "model.add(layers.RepeatVector(ts))\n",
    "model.add(layers.LSTM(64, activation='tanh', return_sequences=True))\n",
    "model.add(layers.LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dense(79)))\n",
    "# activation = relu로 하면 lstm이 gpu로 안돌아가네요..\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch = 500, \n",
    "                     epochs=20, validation_data = valid_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation data는 data가 별로 안커서 직접 저장이 되더라구요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(values, time_steps=ts):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    # Convert 2D sequences into 3D as we will be feeding this into\n",
    "    # a convolutional layer.\n",
    "    return np.expand_dims(output, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1 = create_sequences(VALID_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1 = valid1.reshape((valid1.shape[0],valid1.shape[1],valid1.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(valid1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
